---
title: 'Fine-tuning a Huggingface Model Using Own Data'
date: 2022-02-20
permalink: /posts/2022/02/blog-post-1/
tags:

---

This is my first blog about how to use huggingface models.


### First of all, import all the packages


```python
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import DataCollatorWithPadding
from transformers import AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
import pandas as pd
from datasets import Dataset
import datasets
from datasets import load_metric
import numpy as np
```

### Because of data privacy, using imda as an example

There are many other ways to load dataset. You can also use pickle to read pkl file.

```python
data = pickle.load(open('dataset.pkl','rb'))
```

If you get an error like this:

```
Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks' from '/dartfs-hpc/rc/home/y/f005dcy/.conda/envs/jupyter/lib/python3.7/site-packages/pandas/core/internals/blocks.py'>
```

Then you can try to upgrade your pandas at first.

Use: 

```
pip install --upgrade pandas
```


```python
imdb = load_dataset("imdb")
```

    Reusing dataset imdb (/dartfs-hpc/rc/home/y/f005dcy/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)



      0%|          | 0/3 [00:00<?, ?it/s]


### Transform dataset to pandas dataframe, and then transform back

We can transfrom the contents of this dataset to dataframes. And then use 
```python
train_dataset = Dataset.from_pandas(df_imdb_train)
test_dataset = Dataset.from_pandas(df_imdb_test)
```
to transform them back to dataset format.

Notice: sometime you might need remove the index column, otherwise there will be one extra column which is the index.

```python
train_dataset.remove_columns_('__index_level_0__')
test_dataset.remove_columns_('__index_level_0__')
```

Then we can use 
```python
imdb = datasets.DatasetDict({"train":train_dataset,"test":test_dataset})
```
to put them back together


```python
df_imdb_train = pd.DataFrame(imdb['train'])
df_imdb_test = pd.DataFrame(imdb['test'])
train_dataset = Dataset.from_pandas(df_imdb_train)
test_dataset = Dataset.from_pandas(df_imdb_test)
# train_dataset.remove_columns_('__index_level_0__')
# test_dataset.remove_columns_('__index_level_0__')
imdb = datasets.DatasetDict({"train":train_dataset,"test":test_dataset})
```

### Save the dataset local


```python
imdb.save_to_disk("dataset_from_pd")
imdb = datasets.load_from_disk("dataset_from_pd")
```

### Run an example model


```python
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
```


```python
def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True, padding=True, max_length=512)
```


```python
tokenized_imdb = imdb.map(preprocess_function, batched=True)
```


      0%|          | 0/25 [00:00<?, ?ba/s]



      0%|          | 0/25 [00:00<?, ?ba/s]


### We can see there are two more columns, which are 'input_ids' and 'attention_mask'


```python
tokenized_imdb
```




    DatasetDict({
        train: Dataset({
            features: ['text', 'label', 'input_ids', 'attention_mask'],
            num_rows: 25000
        })
        test: Dataset({
            features: ['text', 'label', 'input_ids', 'attention_mask'],
            num_rows: 25000
        })
    })




```python
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```


```python
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
```

    Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']
    - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
    Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



```python
training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()
```

    PyTorch: setting up devices
    The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
    The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.
    /dartfs-hpc/rc/home/y/f005dcy/.conda/envs/jupyter/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
      FutureWarning,
    ***** Running training *****
      Num examples = 25000
      Num Epochs = 5
      Instantaneous batch size per device = 16
      Total train batch size (w. parallel, distributed & accumulation) = 16
      Gradient Accumulation steps = 1
      Total optimization steps = 7815



Here, if you get an error like this:

```
Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.
```

One possible reason might be, your lables is not standard. This means your lable might be string or other format.

You can try this first.

```python
dataset['train']['label'] = dataset['train']['label'].replace(list(set(dataset['train']['label'])),list(range(len(list(set(dataset['train']['label'])))))) 
```

### And we can test the accuracy

First use this to load your model

```python
model = AutoModelForSequenceClassification.from_pretrained("./results/checkpoint-7500/")
```


```python
metric = load_metric("accuracy")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```


```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    compute_metrics=compute_metrics,
)
trainer.evaluate()
```

    The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text.
    ***** Running Evaluation *****
      Num examples = 25000
      Batch size = 16




<div>

  <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [1563/1563 02:25]
</div>






    {'eval_loss': 0.3616120219230652,
     'eval_accuracy': 0.9326,
     'eval_runtime': 145.9329,
     'eval_samples_per_second': 171.312,
     'eval_steps_per_second': 10.71}

